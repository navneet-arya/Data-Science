{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train : (233154, 41)\n",
      "Shape of Test : (112392, 40)\n"
     ]
    }
   ],
   "source": [
    "# importing lib \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "\n",
    "# Reading the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_original=test.copy()\n",
    "train_original = train.copy()\n",
    "\n",
    "target = 'loan_default'\n",
    "IDcol = 'UniqueID'\n",
    "\n",
    "# getting the shapes of the datasets\n",
    "print(\"Shape of Train :\", train.shape)\n",
    "print(\"Shape of Test :\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values left in the train set: 0\n",
      "Null values left in the test set: 0\n"
     ]
    }
   ],
   "source": [
    "# filling the missing values in the Employment.Type attribute of train and test sets\n",
    "\n",
    "# Employement Type has two types of Employment i.e., self employed and salaried\n",
    "# but the empty values must be the people who don't  work at all that's why it is empty\n",
    "# let's fill unemployed in the place of Null values\n",
    "\n",
    "train['Employment.Type'].fillna('Unemployed', inplace = True)\n",
    "test['Employment.Type'].fillna('Unemployed', inplace = True)\n",
    "\n",
    "# let's check if there is any null values still left or not\n",
    "print(\"Null values left in the train set:\", train.isnull().sum().sum())\n",
    "print(\"Null values left in the test set:\", test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233154, 40)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's save the unique id of the test set and labels set\n",
    "\n",
    "unique_id = test['UniqueID']\n",
    "y_train = train.iloc[:, -1]\n",
    "\n",
    "# let's delete the last column from the dataset to  concat train and test\n",
    "train = train.drop(['loan_default'], axis = 1)\n",
    "\n",
    "# shape of train\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345546, 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets concat the train and test sets for preprocessing and visualizations\n",
    "\n",
    "data = pd.concat([train, test], axis = 0)\n",
    "\n",
    "# let's check the shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    187429\n",
       "1    147013\n",
       "0     11104\n",
       "Name: Employment.Type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encodings for type of employments\n",
    "\n",
    "data['Employment.Type'] = data['Employment.Type'].replace(('Self employed', 'Salaried', 'Unemployed'), (2, 1, 0))\n",
    "\n",
    "# checking the values  of employement type\n",
    "data['Employment.Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing log transformations on disbursed amount, ltv, and asset cost\n",
    "\n",
    "data['disbursed_amount'] = np.log1p(data['disbursed_amount'])\n",
    "data['ltv'] = np.log1p(data['ltv'])\n",
    "data['asset_cost'] = np.log1p(data['asset_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date of birth is an useless attribute \n",
    "#  the only thing we can extract the is the year of birth\n",
    "# let's first convert the date into date-time format\n",
    "\n",
    "data['Date.of.Birth'] = pd.to_datetime(data['Date.of.Birth'], errors = 'coerce')\n",
    "\n",
    "# extracting the year of birth of the customers\n",
    "data['Year_of_birth'] = data['Date.of.Birth'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the credit history format from ayrsbmonths to years \n",
    "# as no. of years are more important\n",
    "\n",
    "data['CREDIT.HISTORY.LENGTH'] = data['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split(' ')[0])\n",
    "data['CREDIT.HISTORY.LENGTH'] = data['CREDIT.HISTORY.LENGTH'].apply(lambda x: x.split('yrs')[0]).astype(np.float64)\n",
    "#data['CREDIT.HISTORY.LENGTH'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the average account age format from ayrsbmonths to years \n",
    "# as no. of years are more important\n",
    "\n",
    "data['AVERAGE.ACCT.AGE'] = data['AVERAGE.ACCT.AGE'].apply(lambda x: x.split(' ')[0])\n",
    "data['AVERAGE.ACCT.AGE'] = data['AVERAGE.ACCT.AGE'].apply(lambda x: x.split('yrs')[0]).astype(np.float64)\n",
    "#data['AVERAGE.ACCT.AGE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply log transformations on EMI Amount of the Primary Loan and Secondary loan\n",
    "\n",
    "data['PRIMARY.INSTAL.AMT'] = np.log1p(data['PRIMARY.INSTAL.AMT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  applying log transformations to the primary account attributes\n",
    "\n",
    "data['PRI.NO.OF.ACCTS'] = np.log1p(data['PRI.NO.OF.ACCTS'])\n",
    "data['PRI.ACTIVE.ACCTS'] = np.log1p(data['PRI.ACTIVE.ACCTS'])\n",
    "data['PRI.OVERDUE.ACCTS'] = np.log1p(data['PRI.OVERDUE.ACCTS'])\n",
    "#data['PRI.CURRENT.BALANCE'] = np.log1p(data['PRI.CURRENT.BALANCE'])\n",
    "#data['PRI.SANCTIONED.AMOUNT'] = np.log1p(data['PRI.SANCTIONED.AMOUNT'])\n",
    "data['PRI.DISBURSED.AMOUNT'] = np.log1p(data['PRI.DISBURSED.AMOUNT'])\n",
    "\n",
    "\n",
    "#  filling  missing values in sec.current.balance\n",
    "data['PRI.CURRENT.BALANCE'].fillna(data['PRI.CURRENT.BALANCE'].mean(), inplace = True)\n",
    "data['PRI.SANCTIONED.AMOUNT'].fillna(data['PRI.SANCTIONED.AMOUNT'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings for bureau score(perform cns score distribution)\n",
    "\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('No Bureau History Available', 0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Sufficient History Not Available', 0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Not Enough Info available on the customer', 0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Activity seen on the customer (Inactive)',0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Updates available in last 36 months', 0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Only a Guarantor', 0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: More than 50 active Accounts found',0)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('M-Very High Risk', 1)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('L-Very High Risk', 1)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('K-High Risk', 2)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('J-High Risk', 2)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('I-Medium Risk', 3)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('H-Medium Risk', 3)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('G-Low Risk', 4)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('F-Low Risk', 4)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('E-Low Risk', 4)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('D-Very Low Risk', 5)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('C-Very Low Risk', 5)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('B-Very Low Risk', 5)\n",
    "data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('A-Very Low Risk', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PERFORM_CNS.SCORE'] = np.log1p(data['PERFORM_CNS.SCORE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract features from disbursal dates\n",
    "# as all  the disbursement dates are of year 2018 so we can extract the months\n",
    "\n",
    "data['DisbursalDate'] = pd.to_datetime(data['DisbursalDate'], errors = 'coerce')\n",
    "\n",
    "# extracting the month of the disbursement\n",
    "data['DisbursalMonth'] = data['DisbursalDate'].dt.month\n",
    "\n",
    "data['DisbursalMonth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing unnecassary columns\n",
    "\n",
    "data = data.drop(['UniqueID', 'supplier_id', 'Current_pincode_ID', 'Date.of.Birth', 'DisbursalDate','manufacturer_id',\n",
    "                  'Aadhar_flag', 'PAN_flag', 'VoterID_flag','Driving_flag', 'Passport_flag',\n",
    "                  'SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS','SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT',\n",
    "       'SEC.DISBURSED.AMOUNT','PERFORM_CNS.SCORE','Employee_code_ID','disbursed_amount', 'asset_cost', 'ltv','PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS',\n",
    "       'PRI.OVERDUE.ACCTS', 'PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT','MobileNo_Avl_Flag','NEW.ACCTS.IN.LAST.SIX.MONTHS',\n",
    "       'PRI.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT','DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS'], axis = 1)\n",
    "\n",
    "# checking the new columns of data\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating train and test datasets from data\n",
    "\n",
    "x_train = data.iloc[:233154,:]\n",
    "x_test = data.iloc[233154:,:]\n",
    "\n",
    "# checking the shape of train and test\n",
    "print(\"Shape of train :\", x_train.shape)\n",
    "print(\"Shape of test :\", x_test.shape)\n",
    "\n",
    "# applying SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "x_resample, y_resample = SMOTE().fit_sample(x_train, y_train.values.ravel()) \n",
    "\n",
    "# checking the shape of x_resample and y_resample\n",
    "print(\"Shape of x:\", x_resample.shape)\n",
    "print(\"Shape of y:\", y_resample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and valid sets from train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_resample, y_resample, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# checking the shapes\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# applying standardization\n",
    "\n",
    "# standardization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_valid = sc.transform(x_valid)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 9)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_valid = pca.transform(x_valid)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(explained_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = xgb.XGBClassifier(n_estimators=150)\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "y_pred=classifier.predict(x_valid)\n",
    "ac=accuracy_score(y_pred,y_valid)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10)\n",
    "print(accuracies.mean())\n",
    "print(accuracies.std())\n",
    "print(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I defined a function which will help us to create xgboost models\n",
    "# # and performs cross validation\n",
    "# def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "#     if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "#         xgtrain = xgb.DMatrix(dtrain[predictors].values,label=dtrain[target].values)\n",
    "#         cvresult = xgb.cv(xgb_param,xgtrain,num_boost_round=alg.get_params()['n_estimators'],\n",
    "#                           nfold=cv_folds,metrics='auc',early_stopping_rounds=early_stopping_rounds)\n",
    "#         alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "# # Fit the algo on the data\n",
    "#     alg.fit(dtrain[predictors],dtrain['loan_default'],eval_metric='auc')\n",
    "    \n",
    "# #     Predict Training set:\n",
    "#     dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "#     dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "    \n",
    "#     #Print model report:\n",
    "#     print (\"\\nModel Report\")\n",
    "#     print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['loan_default'].values, dtrain_predictions))\n",
    "#     print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['loan_default'], dtrain_predprob))\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose all predictors except target and ID col\n",
    "# predictors = [x for x in train.columns if x not in [target,IDcol]]\n",
    "# xgb2 = xgb.XGBClassifier(learning_rate=0.1,n_estimators=1000,max_depth=5,min_child_weight=1,\n",
    "#                     gamma=0,subsample=0.8,colsample_bytree=0.8,objective='binary:logistic',nthread=4,\n",
    "#                     scale_pos_weight=1,seed=27)\n",
    "# modelfit(xgb2,train,predictors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
